# ğŸ©º Kaggle â€“ Diabetes Prediction Competition

![Python](https://img.shields.io/badge/Python-3.10%2B-blue)
![LightGBM](https://img.shields.io/badge/LightGBM-Gradient%20Boosting-green)
![CatBoost](https://img.shields.io/badge/CatBoost-Boosting-orange)
![XGBoost](https://img.shields.io/badge/XGBoost-Tree%20Boosting-red)
![Optuna](https://img.shields.io/badge/Optuna-Hyperparameter%20Tuning-purple)
![Status](https://img.shields.io/badge/Competition-Active-success)
![License](https://img.shields.io/badge/License-MIT-lightgrey)

---

## ğŸ“Œ Overview

This repository contains my **end-to-end machine learning pipeline** for the **Kaggle Diabetes Prediction** competition.  
The objective is to predict the likelihood of diabetes using **demographic, lifestyle, and clinical features**.

I built:

- Strong single-model baselines
- Multiple gradient boosting models
- Advanced ensembling system
- Pseudo-labeling & meta-stacking
- Final rank-based blending strategy

---

## ğŸ† Leaderboard Performance

| Metric | Score |
|--------|-------|
| **Best Public Leaderboard Score** | **0.69869** |
| Models Used | LGBM, CatBoost, XGBoost, Blending |

> Currently aiming for Top-3 finish.

---

## ğŸ§  Machine Learning Methods Used

- LightGBM (baseline & tuned)
- XGBoost with LR decay
- CatBoost categorical boosting
- Target Encoding for high-cardinality variables
- Pseudo-Labeling (semi-supervised learning)
- Model Stacking (meta learner)
- Rank Averaging & Weighted Blending
- Optuna Bayesian hyperparameter tuning
- Adversarial validation to detect leakage & drift

---

## ğŸ§° Tech Stack

- Python
- Pandas / NumPy
- LightGBM
- XGBoost
- CatBoost
- Optuna
- Scikit-learn
- Matplotlib / Seaborn (EDA)

---

## ğŸ“‚ Repository Structure

```
kaggle_diabetes_competition/
â”‚
â”œâ”€â”€ train.csv
â”œâ”€â”€ test.csv
â”‚
â”œâ”€â”€ optuna_lgbm_fast.py
â”œâ”€â”€ model_lightgbm.py
â”œâ”€â”€ model_xgboost_fast.py
â”œâ”€â”€ model_catboost_fast.py
â”‚
â”œâ”€â”€ stacking_simple.py
â”œâ”€â”€ stacking_meta_fast.py
â”œâ”€â”€ blend_top_models.py
â”œâ”€â”€ final_super_blend.py
â”‚
â”œâ”€â”€ pseudo_label_v2.py
â”‚
â””â”€â”€ README.md
```

---

## ğŸš€ Reproducibility â€“ How to Run

```
pip install -r requirements.txt
python optuna_lgbm_fast.py
python model_catboost_fast.py
python model_xgboost_fast.py
python stacking_meta_fast.py
python final_super_blend.py
```

---

## ğŸ§¾ Results Summary

| Model | Score |
|------|------|
| Optuna + LGBM | 0.697 |
| CatBoost tuned | 0.697 |
| XGBoost tuned | 0.694 |
| Target Encoding + LGBM | 0.697 |
| Meta-Stacking | 0.6973 |
| **Final Blend** | **0.69869** |

---

## ğŸ“ˆ Key Insights

- Categorical encoding and stacked blending provide major lift
- Simple models with smart ensembling outperform deep nets
- Pseudo-labeling improves generalization
- Public LB variance is significant â€” rank-based blending stabilizes score

---

## ğŸ‘¨â€ğŸ’» Author

**Tonumay Bhattacharya**  
ğŸ“ India  

---

## ğŸ“ License

This project is licensed under the MIT License.
